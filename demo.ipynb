{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syncnet DEMO\n",
    "\n",
    "https://github.com/voletiv/syncnet-in-keras/\n",
    "\n",
    "1. Given a video (*.mp4), convert to proper input format to the Syncnet lip & audio models\n",
    "2. Load the Syncnet lip and audio models\n",
    "3. Calculate lip-video and audio embeddings using Syncnet\n",
    "4. Calculate Euclidian distance between the lip and audio embeddings to check if video/audio are in sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io.wavfile as wav\n",
    "import speechpy\n",
    "\n",
    "import syncnet_params\n",
    "\n",
    "from syncnet_functions import load_pretrained_syncnet_model\n",
    "\n",
    "# DETECT MOUTH IN FRAME USING process_lrw_functions\n",
    "# FROM https://github.com/voletiv/lipreading-in-the-wild-experiments/tree/master/process-lrw\n",
    "import sys\n",
    "sys.path.append('../lipreading-in-the-wild-experiments/process-lrw')\n",
    "from process_lrw_functions import detect_mouth_in_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_syncnet_lip_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syncnet_lip_model_input(video, shape_predictor_path=\"shape_predictor_68_face_landmarks.dat\"):\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "\n",
    "    cap         = cv2.VideoCapture(video)\n",
    "    frameFPS    = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frameCount  = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frameWidth  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(\"FPS: {}\".format(frameFPS))\n",
    "    print(\"Frames: {}\".format(frameCount))\n",
    "    print(\"Width: {}\".format(frameWidth))\n",
    "    print(\"Height: {}\".format(frameHeight))\n",
    "\n",
    "    # Default face rect\n",
    "    face = dlib.rectangle(30, 30, 220, 220)\n",
    "\n",
    "    lip_model_input = []\n",
    "\n",
    "    frame_index = 0\n",
    "\n",
    "    # Read frames from the video\n",
    "    while(cap.isOpened()):\n",
    "\n",
    "        frames = []\n",
    "        for i in range(5):\n",
    "        \n",
    "            _, frame = cap.read()\n",
    "            frame_index += 1\n",
    "            # print(\"Frame\", frame_index+1, \"of\", frameCount, end=\"\\r\")\n",
    "\n",
    "            # If no frame is read, break\n",
    "            if frame is None:\n",
    "                break\n",
    "\n",
    "            # Detect mouth in the frame\n",
    "            mouth, _ = detect_mouth_in_frame(frame, detector, predictor,\n",
    "                                                prevFace=face, verbose=False)\n",
    "\n",
    "            # Convert mouth to grayscale\n",
    "            mouth = cv2.cvtColor(mouth, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Resize mouth to syncnet input shape\n",
    "            mouth = cv2.resize(mouth, (syncnet_params.MOUTH_H, syncnet_params.MOUTH_W))\n",
    "\n",
    "            # Subtract 110 from all mouth values (Checked in syncnet_demo.m)\n",
    "            mouth = mouth - 110.\n",
    "\n",
    "            frames.append(mouth)\n",
    "\n",
    "        if len(frames) == 5:\n",
    "            stacked = np.stack(frames, axis=-1) #syncnet requires (112,112,5)\n",
    "            lip_model_input.append(stacked)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return np.array(lip_model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_syncnet_audio_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_syncnet_mfcc(wav_file, verbose=False):\n",
    "    \"\"\"To extract mfcc features of audio clips 0.2 seconds in length each,\n",
    "    i.e. of 20 MFCC features in each clip (acc. to syncnet paper)\n",
    "    Output mfcc_clips shape === (N, 12, 20, 1),\n",
    "    where N = len(mfcc_features) // 20\n",
    "    \"\"\"\n",
    "\n",
    "    rate, sig = wav.read(wav_file)\n",
    "    if verbose:\n",
    "        print(\"Sig length: {}, sample_rate: {}\".format(len(sig), rate))\n",
    "\n",
    "    try:\n",
    "        mfcc_features = speechpy.feature.mfcc(sig, sampling_frequency=rate, frame_length=0.010, frame_stride=0.010)\n",
    "    except IndexError:\n",
    "        raise ValueError(\"ERROR: Index error occurred while extracting mfcc\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"mfcc_features shape:\", mfcc_features.shape)\n",
    "\n",
    "    # Number of audio clips = len(mfcc_features) // length of each audio clip\n",
    "    number_of_audio_clips = len(mfcc_features) // syncnet_params.AUDIO_TIME_STEPS\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of audio clips:\", number_of_audio_clips)\n",
    "\n",
    "    # Don't consider the first MFCC feature, only consider the next 12 (Checked in syncnet_demo.m)\n",
    "    # Also, only consider syncnet_params.AUDIO_TIME_STEPS*number_of_audio_clips features\n",
    "    mfcc_features = mfcc_features[:syncnet_params.AUDIO_TIME_STEPS*number_of_audio_clips, 1:]\n",
    "\n",
    "    # Reshape mfcc_features from (x, 12) to (x//20, 12, 20, 1)\n",
    "    mfcc_features = np.expand_dims(np.transpose(np.split(mfcc_features, number_of_audio_clips), (0, 2, 1)), axis=-1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Final mfcc_features shape:\", mfcc_features.shape)\n",
    "\n",
    "    return mfcc_features\n",
    "\n",
    "\n",
    "def get_syncnet_audio_model_input(video):\n",
    "\n",
    "    # Convert video's audio to .wav file\n",
    "    audio_out = \"{}.wav\".format(video)\n",
    "    command = \"ffmpeg -y -loglevel panic -i {} -acodec pcm_s16le -ac 1 -ar 16000 {}\".format(video, audio_out)\n",
    "    os.system(command)\n",
    "\n",
    "    # Extract proper input to syncnet_audio_model\n",
    "    return extract_syncnet_mfcc(audio_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Given a video, convert to proper inputs to the Syncnet lip & audio models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure video is of 25fps!**\n",
    "If not, use the following ffmpeg command to convert fps:\n",
    "\n",
    "```\n",
    "ffmpeg -i <video>.mp4 -r 25 -y <video_at_25_fps>.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_video_to_25_fps(video):\n",
    "    cmd = \"ffmpeg -i {} -r 25 -y tmp.mp4\".format(video)\n",
    "    os.system(cmd)\n",
    "    cmd = \"mv tmp.mp4 {}\".format(video)\n",
    "    os.system(cmd)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_test = \"test/unsynced.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_video_to_25_fps(video_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_input = get_syncnet_lip_model_input(video_to_test)\n",
    "print(lip_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "audio_input = get_syncnet_audio_model_input(video_to_test)\n",
    "print(audio_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Syncnet lip and audio models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v4'\n",
    "mode = 'both'\n",
    "syncnet_audio_model, syncnet_lip_model = load_pretrained_syncnet_model(version=version, mode=mode, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(syncnet_audio_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(syncnet_lip_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate lip-video and audio embeddings using Syncnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings = syncnet_audio_model.predict(audio_input)\n",
    "print(audio_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_embeddings = syncnet_lip_model.predict(lip_input)\n",
    "print(lip_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Calculate Euclidian distance between the lip and audio embeddings to check if video/audio are in sync\n",
    "\n",
    "1. Pass the audio frame through the audio model to get its encoding (a 128-dimensional feature), pass the video frame through the lip model to get its encoding (a 128-dimensional features)\n",
    "\n",
    "2. Check the euclidean distance between the audio encoding and the video encoding.\n",
    "\n",
    "3. If the distance is greater than a threshold, then it is said that audio frame and that video frame are not in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance_(np_data_1, np_data_2): \n",
    "    dist = np.sqrt( np.sum(np.square(np_data_1 - np_data_2), axis=-1) )\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = euclidian_distance(audio_embeddings, lip_embeddings)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
